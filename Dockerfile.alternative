# Alternative Dockerfile approaches for different use cases

# Option 1: Use official PyTorch image as base
FROM pytorch/pytorch:2.0.1-cuda11.7-cudnn8-runtime

# Set work directory
WORKDIR /app

# Copy requirements and install
COPY requirements.txt .
RUN pip install --no-cache-dir --upgrade pip
RUN pip install --no-cache-dir -r requirements.txt

# Copy handler
COPY handler.py .

# Create cache directories
RUN mkdir -p /root/.cache/huggingface

CMD ["python", "-u", "handler.py"]

# Option 2: If you have a custom Chatterbox image
# FROM your-custom-chatterbox-image:latest
# 
# # Add our serverless handler
# COPY handler.py .
# COPY requirements-runpod.txt .
# RUN pip install --no-cache-dir -r requirements-runpod.txt
# 
# CMD ["python", "-u", "handler.py"]

# Option 3: Multi-stage build for smaller final image
# FROM nvidia/cuda:11.8-devel-ubuntu22.04 AS builder
# 
# # Install build dependencies
# RUN apt-get update && apt-get install -y python3 python3-pip git
# 
# # Build stage
# WORKDIR /build
# COPY requirements.txt .
# RUN pip install --no-cache-dir --user -r requirements.txt
# 
# # Runtime stage
# FROM nvidia/cuda:11.8-runtime-ubuntu22.04
# 
# # Copy built packages
# COPY --from=builder /root/.local /root/.local
# 
# # Copy application
# WORKDIR /app
# COPY handler.py .
# 
# # Make sure to use local packages
# ENV PATH=/root/.local/bin:$PATH
# 
# CMD ["python", "-u", "handler.py"] 